---
title: Int8 ViT on TVM, 1.5 speed up compared with TensorRT
subtitle: 

# Summary for listings and search engines
summary: The first to support Int8 ViT for TVM, achieving a significant speed up.

# Link this post with a project
projects: []

# Date published
date: "2021-04-19T00:00:00Z"

# Date updated
lastmod: "2021-04-19T00:00:00Z"

# Is this an unpublished draft?
draft: false

# Show this page in the Featured widget?
featured: true

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 
  focal_point: ""
  placement: 2
  preview_only: false

authors:
- ruihao-gong

tags:
- Transformer
- Quantization

categories:
- Deep learning compiler
- Quantization
- Transformer
---

For details, please refer to [Link](https://mp.weixin.qq.com/s/BtxmGr2YUyY1zDdL4sCJ9w).
